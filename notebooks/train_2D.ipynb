{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model contains all the information required to train all four different models. This uses two main python files: \n",
    "- `utils_2D`: Contains all the utility functions required for training the models\n",
    "- `Kits2019_2D`: Contains the class for the dataset (**Kits20192DDataset**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a35TRluPFmUj",
    "outputId": "b297a55a-ace7-4c3a-f86b-2c2a487b8b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: torchaudio==2.3.0 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
      "Requirement already satisfied: torchvision==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.0) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
      "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
      "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.10.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.6)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
      "Requirement already satisfied: resnest in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
      "Requirement already satisfied: geffnet in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.10/dist-packages (0.7.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.2)\n",
      "Requirement already satisfied: monai in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
      "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from resnest) (1.26.4)\n",
      "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from resnest) (1.3.7)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from resnest) (2.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from resnest) (1.13.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from resnest) (2.32.3)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from geffnet) (0.18.0)\n",
      "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels) (4.0.0)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.6 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.26.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.16.0)\n",
      "Requirement already satisfied: timm==0.9.7 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.9.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->resnest) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->resnest) (12.6.77)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->resnest) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->resnest) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->resnest) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->resnest) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->resnest) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->resnest) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.0 torchaudio==2.3.0 torchvision==0.18.0\n",
    "!pip install albumentations numpy pandas scikit_learn kaggle\n",
    "!pip install resnest geffnet opencv-python pretrainedmodels tqdm Pillow packaging monai segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1AK0H2oT0F7"
   },
   "source": [
    "Training the U-nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyw1Zn33Vq1M",
    "outputId": "fa302a92-ae92-42c2-b9c3-8abaa1a3ea5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import monai.networks.nets as monai_nets\n",
    "\n",
    "import utils_2D as u\n",
    "from Kits2019_2D import Kits20192DDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpAxpGnOFmUk",
    "outputId": "691ad237-8671-4378-f624-8ef05b0b1909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fuIWnk8mDEf4",
    "outputId": "6772162e-099a-4fd0-aef8-5ab25232064e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/MyDrive/flattened_data.zip\n",
      "replace flattened_data/flattened_data/images/image_000000.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
     ]
    }
   ],
   "source": [
    "!unzip /content/drive/MyDrive/flattened_data.zip -d flattened_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUOoTcoRFHEM"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZXBKxQx4FmUl"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Train a MONAI model for medical image segmentation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The MONAI model to train (e.g., UNet, AttentionUNet, SwinUNETR).\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    config : dict\n",
    "        Configuration dictionary containing:\n",
    "        - \"device\" (str): Device to train on (\"cuda\" or \"cpu\").\n",
    "        - \"lr\" (float): Learning rate.\n",
    "        - \"epochs\" (int): Number of training epochs.\n",
    "        - \"checkpoint_path\" (str): Path to save the best model checkpoint.\n",
    "        - \"num_classes\" (int): Number of segmentation classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    # Wrap model with DataParallel if multiple GPUs are available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    checkpoint_path = config[\"checkpoint_path\"]\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).long().squeeze(1)  # Fixed typo in squeeze\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']} - Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device).long().squeeze(1)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model with the best validation loss\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            # Save the model state dict (handle DataParallel wrapper)\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                torch.save(model.module.state_dict(), checkpoint_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Model saved with validation loss: {best_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iR36AilaFmUm"
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, config):\n",
    "    \"\"\"\n",
    "    Create DataLoaders with multi-GPU support\n",
    "    \"\"\"\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = 4 * torch.cuda.device_count() if torch.cuda.is_available() else 4  # Scale workers with GPUs\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True  # Enables faster data transfer to GPU\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3M5jRFKPDEf5"
   },
   "outputs": [],
   "source": [
    "def select_model(model_name, config):\n",
    "    \"\"\"\n",
    "    Initialize and return the segmentation model.\n",
    "    \"\"\"\n",
    "    if model_name == \"UNet\":\n",
    "        model = smp.Unet(\n",
    "            encoder_name=\"resnet34\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=config[\"in_channels\"],\n",
    "            classes=config[\"num_classes\"]\n",
    "        )\n",
    "    elif model_name == \"UNetV2\":\n",
    "        model = monai_nets.UNet(\n",
    "            spatial_dims=config[\"spatial_dims\"],\n",
    "            in_channels=config[\"in_channels\"],\n",
    "            out_channels=config[\"out_channels\"],\n",
    "            channels=config[\"channels\"],\n",
    "            strides=config[\"strides\"]\n",
    "        )\n",
    "    elif model_name == \"AttentionUNet\":\n",
    "        model = monai_nets.AttentionUnet(\n",
    "        spatial_dims=config[\"spatial_dims\"],\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        out_channels=config[\"out_channels\"],\n",
    "        channels=config[\"channels\"],\n",
    "        strides=config[\"strides\"]\n",
    "    )\n",
    "    elif model_name == \"SwinUNETR\":\n",
    "        model = monai_nets.SwinUNETR(\n",
    "        spatial_dims=config[\"spatial_dims\"],\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        out_channels=config[\"out_channels\"],\n",
    "        img_size=config[\"img_size\"],\n",
    "        feature_size=config[\"feature_size\"],\n",
    "    )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MIFVPeX3DEf5"
   },
   "outputs": [],
   "source": [
    "def check_mask_values(dataset):\n",
    "    \"\"\"Check unique values in masks\"\"\"\n",
    "    all_unique = set()\n",
    "    for i in range(len(dataset)):\n",
    "        image, mask = dataset[i]\n",
    "        unique_values = torch.unique(mask).cpu().numpy()\n",
    "        all_unique.update(unique_values)\n",
    "    print(f\"All unique values in masks: {sorted(all_unique)}\")\n",
    "    return all_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "32UKnJkfDEf5"
   },
   "outputs": [],
   "source": [
    "selected_model_name = \"SwinUNETR\"\n",
    "config = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"checkpoint_path\": f\"best_{selected_model_name}.pth\",\n",
    "    \"num_classes\": 3,\n",
    "    \"in_channels\": 1,\n",
    "    \"batch_size\": 16 * torch.cuda.device_count() if torch.cuda.is_available() else 16,\n",
    "    # Add these parameters\n",
    "    \"image_dir\": \"flattened_data/images\",  # Update this path to your image directory\n",
    "    \"mask_dir\": \"flattened_data/masks\",    # Update this path to your mask directory\n",
    "    \"split_train\": 0.7,\n",
    "    \"split_val\": 0.15,\n",
    "    \"split_test\": 0.15,\n",
    "    \"image_size\": 256,\n",
    "    \"feature_size\": 48,\n",
    "    \"spatial_dims\": 2,\n",
    "    \"img_size\": (256, 256),\n",
    "    \"out_channels\": 3,\n",
    "    \"channels\":(16, 32, 64, 128, 256),\n",
    "    \"strides\":(2, 2, 2, 2)\n",
    "}\n",
    "\n",
    "config[\"image_dir\"] = \"/content/flattened_data/flattened_data/images\"\n",
    "config[\"mask_dir\"] = \"/content/flattened_data/flattened_data/masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CIj7tWjgDEf5",
    "outputId": "3477bfc2-b170-43cd-bc93-a2943028df7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16336 images and 16336 masks.\n",
      "First image path: /content/flattened_data/flattened_data/images/image_000000.npy\n",
      "First mask path: /content/flattened_data/flattened_data/masks/mask_000000.npy\n"
     ]
    }
   ],
   "source": [
    "image_paths, mask_paths = u.build_dataset_paths(\n",
    "    images_dir=config[\"image_dir\"],\n",
    "    masks_dir=config[\"mask_dir\"],\n",
    "    image_ext=\".npy\",  # Update if using different extension\n",
    "    mask_ext=\".npy\"    # Update if using different extension\n",
    ")\n",
    "print(f\"Found {len(image_paths)} images and {len(mask_paths)} masks.\")\n",
    "print(f\"First image path: {image_paths[0]}\")\n",
    "print(f\"First mask path: {mask_paths[0]}\")\n",
    "# Update config with the paths\n",
    "config[\"image_paths\"] = image_paths\n",
    "config[\"mask_paths\"] = mask_paths\n",
    "\n",
    "\n",
    "# files.download(f\"best_{selected_model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MTJlQZV7Zinh"
   },
   "outputs": [],
   "source": [
    "# Continue with the rest of your code\n",
    "train_dataset, val_dataset, test_dataset = u.prepare_datasets(config)\n",
    "\n",
    "\n",
    "# Assuming train_dataset, val_dataset, and test_dataset are already defined\n",
    "train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, config)\n",
    "\n",
    "\n",
    "# print(\"Checking mask values...\")\n",
    "# unique_train = check_mask_values(train_dataset)\n",
    "# unique_val = check_mask_values(val_dataset)\n",
    "# unique_test = check_mask_values(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WA3WTKRUZbYd",
    "outputId": "722ca446-040f-4d11-8f90-8335df32e678"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training Loss: 0.1093\n",
      "Epoch 1/20 - Validation Loss: 0.0421\n",
      "Model saved with validation loss: 0.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training Loss: 0.0322\n",
      "Epoch 2/20 - Validation Loss: 0.0218\n",
      "Model saved with validation loss: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training Loss: 0.0208\n",
      "Epoch 3/20 - Validation Loss: 0.0152\n",
      "Model saved with validation loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training Loss: 0.0160\n",
      "Epoch 4/20 - Validation Loss: 0.0123\n",
      "Model saved with validation loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training Loss: 0.0137\n",
      "Epoch 5/20 - Validation Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training Loss: 0.0124\n",
      "Epoch 6/20 - Validation Loss: 0.0094\n",
      "Model saved with validation loss: 0.0094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training Loss: 0.0113\n",
      "Epoch 7/20 - Validation Loss: 0.0081\n",
      "Model saved with validation loss: 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training Loss: 0.0106\n",
      "Epoch 8/20 - Validation Loss: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training Loss: 0.0097\n",
      "Epoch 9/20 - Validation Loss: 0.0071\n",
      "Model saved with validation loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training Loss: 0.0093\n",
      "Epoch 10/20 - Validation Loss: 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Training Loss: 0.0091\n",
      "Epoch 11/20 - Validation Loss: 0.0067\n",
      "Model saved with validation loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Training Loss: 0.0082\n",
      "Epoch 12/20 - Validation Loss: 0.0064\n",
      "Model saved with validation loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Training Loss: 0.0081\n",
      "Epoch 13/20 - Validation Loss: 0.0063\n",
      "Model saved with validation loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Training Loss: 0.0076\n",
      "Epoch 14/20 - Validation Loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Training Loss: 0.0076\n",
      "Epoch 15/20 - Validation Loss: 0.0060\n",
      "Model saved with validation loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Training Loss: 0.0073\n",
      "Epoch 16/20 - Validation Loss: 0.0054\n",
      "Model saved with validation loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Training Loss: 0.0072\n",
      "Epoch 17/20 - Validation Loss: 0.0054\n",
      "Model saved with validation loss: 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Training Loss: 0.0067\n",
      "Epoch 18/20 - Validation Loss: 0.0053\n",
      "Model saved with validation loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Training Loss: 0.0068\n",
      "Epoch 19/20 - Validation Loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/albucore/decorators.py:43: UserWarning: HueSaturationValue: hue_shift and sat_shift are not applicable to grayscale image. Set them to 0 or use RGB image\n",
      "  result = func(img, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Training Loss: 0.0066\n",
      "Epoch 20/20 - Validation Loss: 0.0054\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select and initialize the model\n",
    "\n",
    "model = select_model(selected_model_name, config)\n",
    "\n",
    "# Train the selected model\n",
    "trained_model = train_model(model, train_loader, val_loader, config)\n",
    "\n",
    "#If in Google Colab, download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cj3qcA8xYE4i",
    "outputId": "cd89f022-8cd3-40c8-f4b5-a25ed8958583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics by Category:\n",
      "\n",
      "Category: background_kidney\n",
      "Number of Images: 1620\n",
      "  Background:\n",
      "    Mean DSC: 0.999\n",
      "    Mean IoU: 0.999\n",
      "  Kidney:\n",
      "    Mean DSC: 0.943\n",
      "    Mean IoU: 0.911\n",
      "  Tumor:\n",
      "    Mean DSC: 0.000\n",
      "    Mean IoU: 0.000\n",
      "\n",
      "Category: background_kidney_tumor\n",
      "Number of Images: 753\n",
      "  Background:\n",
      "    Mean DSC: 0.999\n",
      "    Mean IoU: 0.998\n",
      "  Kidney:\n",
      "    Mean DSC: 0.920\n",
      "    Mean IoU: 0.865\n",
      "  Tumor:\n",
      "    Mean DSC: 0.622\n",
      "    Mean IoU: 0.553\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "\n",
    "def calculate_metrics(prediction, ground_truth, num_classes=3):\n",
    "    \"\"\"Calculate DSC and IoU for each class, excluding missing classes.\"\"\"\n",
    "    dsc_scores = []\n",
    "    iou_scores = []\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_mask = prediction == class_id\n",
    "        gt_mask = ground_truth == class_id\n",
    "\n",
    "        if not pred_mask.any() and not gt_mask.any():\n",
    "            dsc_scores.append(None)\n",
    "            iou_scores.append(None)\n",
    "            continue\n",
    "\n",
    "        intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "        pred_sum = pred_mask.sum()\n",
    "        gt_sum = gt_mask.sum()\n",
    "\n",
    "        dsc = (2.0 * intersection) / (pred_sum + gt_sum + 1e-7)\n",
    "        dsc_scores.append(dsc)\n",
    "\n",
    "        union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "        iou = intersection / (union + 1e-7)\n",
    "        iou_scores.append(iou)\n",
    "\n",
    "    return dsc_scores, iou_scores\n",
    "\n",
    "\n",
    "def load_model(model_name, config):\n",
    "    \"\"\"Load the trained UNet model.\"\"\"\n",
    "    device = config[\"device\"]\n",
    "    model_path = f\"best_{model_name}.pth\"\n",
    "    model = select_model(model_name, config)\n",
    "    if device == \"cuda\":\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def process_mask(mask, target_size):\n",
    "    \"\"\"Resize and clip mask values.\"\"\"\n",
    "    transform = Compose([Resize(target_size, target_size)])\n",
    "    mask = transform(image=mask)['image']\n",
    "    mask = mask.astype(np.int64)\n",
    "    mask = np.clip(mask, 0, 2)\n",
    "    return mask\n",
    "\n",
    "def process_batch(image_batch, mask_batch, model, config, output_dir=\"results1\"):\n",
    "    \"\"\"Process a batch of images and masks.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = config[\"device\"]\n",
    "    image_size = config[\"image_size\"]\n",
    "\n",
    "    all_dsc_scores = []\n",
    "    all_iou_scores = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (images, masks) in enumerate(zip(image_batch, mask_batch)):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).long().squeeze(1)\n",
    "            print(f\"Shape of images: {images.shape}\")\n",
    "            print(f\"Shape of masks: {masks.shape}\")\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                prediction = predictions[i]\n",
    "                ground_truth = masks[i]\n",
    "\n",
    "                ground_truth = process_mask(ground_truth, image_size)\n",
    "\n",
    "                dsc_scores, iou_scores = calculate_metrics(prediction, ground_truth)\n",
    "                all_dsc_scores.append(dsc_scores)\n",
    "                all_iou_scores.append(iou_scores)\n",
    "\n",
    "                colors = np.array([[0, 0, 0], [0, 255, 0], [255, 0, 0]])\n",
    "                prediction_rgb = colors[prediction]\n",
    "                ground_truth_rgb = colors[ground_truth]\n",
    "\n",
    "                plt.figure(figsize=(20, 8))\n",
    "                plt.subplot(131)\n",
    "                plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "                plt.title('Original Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(132)\n",
    "                plt.imshow(prediction_rgb)\n",
    "                plt.title('Prediction')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(133)\n",
    "                plt.imshow(ground_truth_rgb)\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "\n",
    "                save_path = os.path.join(output_dir, f'result_{idx}_{i}.png')\n",
    "                plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "                plt.close()\n",
    "                print(f\"Saved result to {save_path}\")\n",
    "\n",
    "    return all_dsc_scores, all_iou_scores\n",
    "\n",
    "def identify_classes(mask, num_classes=3):\n",
    "    \"\"\"Identify which classes are present in the mask.\"\"\"\n",
    "    present_classes = set(np.unique(mask))\n",
    "    return {class_id: (class_id in present_classes) for class_id in range(num_classes)}\n",
    "\n",
    "\n",
    "\n",
    "# Load Model\n",
    "model_name = \"SwinUNETR\"\n",
    "model = load_model(model_name, config)\n",
    "\n",
    "\n",
    "# Track metrics for different class combinations\n",
    "class_metrics = {\n",
    "    \"background_kidney\": {\"dsc\": [], \"iou\": [], \"count\": 0},\n",
    "    \"background_kidney_tumor\": {\"dsc\": [], \"iou\": [], \"count\": 0},\n",
    "}\n",
    "\n",
    "# Loop through test_loader\n",
    "for batch_idx, (image_batch, mask_batch) in enumerate(test_loader):\n",
    "    images = image_batch.to(config[\"device\"])\n",
    "    masks = mask_batch.to(config[\"device\"]).long().squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            prediction = predictions[i]\n",
    "            ground_truth = masks[i].cpu().numpy()\n",
    "\n",
    "            # Identify classes present in the ground truth\n",
    "            gt_classes = identify_classes(ground_truth, config[\"num_classes\"])\n",
    "            pred_classes = identify_classes(prediction, config[\"num_classes\"])\n",
    "\n",
    "            # Determine the category of the current image\n",
    "            if gt_classes[1] and not gt_classes[2]:  # Background + Kidney\n",
    "                category = \"background_kidney\"\n",
    "            elif gt_classes[1] and gt_classes[2]:  # Background + Kidney + Tumor\n",
    "                category = \"background_kidney_tumor\"\n",
    "            else:\n",
    "                continue  # Ignore images with only the background\n",
    "\n",
    "            # Process and resize ground truth\n",
    "            ground_truth = process_mask(ground_truth, config[\"image_size\"])\n",
    "\n",
    "            # Calculate metrics\n",
    "            dsc_scores, iou_scores = calculate_metrics(prediction, ground_truth, config[\"num_classes\"])\n",
    "\n",
    "            # Update category metrics\n",
    "            class_metrics[category][\"dsc\"].append(dsc_scores)\n",
    "            class_metrics[category][\"iou\"].append(iou_scores)\n",
    "            class_metrics[category][\"count\"] += 1\n",
    "\n",
    "\n",
    "# Calculate and display metrics\n",
    "print(\"\\nMetrics by Category:\")\n",
    "for category, metrics in class_metrics.items():\n",
    "    if metrics[\"count\"] > 0:\n",
    "        mean_dsc = [\n",
    "            np.nanmean([scores[i] for scores in metrics[\"dsc\"] if scores[i] is not None])\n",
    "            for i in range(config[\"num_classes\"])\n",
    "        ]\n",
    "        mean_iou = [\n",
    "            np.nanmean([scores[i] for scores in metrics[\"iou\"] if scores[i] is not None])\n",
    "            for i in range(config[\"num_classes\"])\n",
    "        ]\n",
    "\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"Number of Images: {metrics['count']}\")\n",
    "\n",
    "        for i, class_name in enumerate(['Background', 'Kidney', 'Tumor']):\n",
    "            print(f\"  {class_name}:\")\n",
    "            print(f\"    Mean DSC: {mean_dsc[i]:.3f}\")\n",
    "            print(f\"    Mean IoU: {mean_iou[i]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyaON9XGIGLY",
    "outputId": "472ac252-c528-4e55-c0b7-261a57402f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved result to results1/result_0_0.png\n",
      "Saved result to results1/result_10_0.png\n",
      "Saved result to results1/result_20_0.png\n",
      "Saved result to results1/result_30_0.png\n",
      "Saved result to results1/result_40_0.png\n",
      "Saved result to results1/result_50_0.png\n",
      "Saved result to results1/result_60_0.png\n",
      "Saved result to results1/result_70_0.png\n",
      "Saved result to results1/result_80_0.png\n",
      "Saved result to results1/result_90_0.png\n",
      "Saved result to results1/result_100_0.png\n",
      "Saved result to results1/result_110_0.png\n",
      "Saved result to results1/result_120_0.png\n",
      "Saved result to results1/result_130_0.png\n",
      "Saved result to results1/result_140_0.png\n",
      "Saved result to results1/result_150_0.png\n",
      "Saved result to results1/result_160_0.png\n",
      "Saved result to results1/result_170_0.png\n",
      "Saved result to results1/result_180_0.png\n",
      "Saved result to results1/result_190_0.png\n",
      "Saved result to results1/result_200_0.png\n",
      "Saved result to results1/result_210_0.png\n",
      "Saved result to results1/result_220_0.png\n",
      "Saved result to results1/result_230_0.png\n",
      "Saved result to results1/result_240_0.png\n",
      "Saved result to results1/result_250_0.png\n",
      "Saved result to results1/result_260_0.png\n",
      "Saved result to results1/result_270_0.png\n",
      "Saved result to results1/result_280_0.png\n",
      "Saved result to results1/result_290_0.png\n",
      "Saved result to results1/result_300_0.png\n",
      "Saved result to results1/result_310_0.png\n",
      "Saved result to results1/result_320_0.png\n",
      "Saved result to results1/result_330_0.png\n",
      "Saved result to results1/result_340_0.png\n",
      "Saved result to results1/result_350_0.png\n",
      "Saved result to results1/result_360_0.png\n",
      "Saved result to results1/result_370_0.png\n",
      "Saved result to results1/result_380_0.png\n",
      "Saved result to results1/result_390_0.png\n",
      "Saved result to results1/result_400_0.png\n",
      "Saved result to results1/result_410_0.png\n",
      "Saved result to results1/result_420_0.png\n",
      "Saved result to results1/result_430_0.png\n",
      "Saved result to results1/result_440_0.png\n",
      "Saved result to results1/result_450_0.png\n",
      "Saved result to results1/result_460_0.png\n",
      "Saved result to results1/result_470_0.png\n",
      "Saved result to results1/result_480_0.png\n",
      "Saved result to results1/result_490_0.png\n",
      "Saved result to results1/result_500_0.png\n",
      "Saved result to results1/result_510_0.png\n",
      "Saved result to results1/result_520_0.png\n",
      "Saved result to results1/result_530_0.png\n",
      "Saved result to results1/result_540_0.png\n",
      "Saved result to results1/result_550_0.png\n",
      "Saved result to results1/result_560_0.png\n",
      "Saved result to results1/result_570_0.png\n",
      "Saved result to results1/result_580_0.png\n",
      "Saved result to results1/result_590_0.png\n",
      "Saved result to results1/result_600_0.png\n",
      "Saved result to results1/result_610_0.png\n",
      "Saved result to results1/result_620_0.png\n",
      "Saved result to results1/result_630_0.png\n",
      "Saved result to results1/result_640_0.png\n",
      "Saved result to results1/result_650_0.png\n",
      "Saved result to results1/result_660_0.png\n",
      "Saved result to results1/result_670_0.png\n",
      "Saved result to results1/result_680_0.png\n",
      "Saved result to results1/result_690_0.png\n",
      "Saved result to results1/result_700_0.png\n",
      "Saved result to results1/result_710_0.png\n",
      "Saved result to results1/result_720_0.png\n",
      "Saved result to results1/result_730_0.png\n",
      "Saved result to results1/result_740_0.png\n",
      "Saved result to results1/result_750_0.png\n",
      "Saved result to results1/result_760_0.png\n",
      "Saved result to results1/result_770_0.png\n",
      "Saved result to results1/result_780_0.png\n",
      "Saved result to results1/result_790_0.png\n",
      "Saved result to results1/result_800_0.png\n",
      "Saved result to results1/result_810_0.png\n",
      "Saved result to results1/result_820_0.png\n",
      "Saved result to results1/result_830_0.png\n",
      "Saved result to results1/result_840_0.png\n",
      "Saved result to results1/result_850_0.png\n",
      "Saved result to results1/result_860_0.png\n",
      "Saved result to results1/result_870_0.png\n",
      "Saved result to results1/result_880_0.png\n",
      "Saved result to results1/result_890_0.png\n",
      "Saved result to results1/result_900_0.png\n",
      "Saved result to results1/result_910_0.png\n",
      "Saved result to results1/result_920_0.png\n",
      "Saved result to results1/result_930_0.png\n",
      "Saved result to results1/result_940_0.png\n",
      "Saved result to results1/result_950_0.png\n",
      "Saved result to results1/result_960_0.png\n",
      "Saved result to results1/result_970_0.png\n",
      "Saved result to results1/result_980_0.png\n",
      "Saved result to results1/result_990_0.png\n",
      "Saved result to results1/result_1000_0.png\n",
      "Saved result to results1/result_1010_0.png\n",
      "Saved result to results1/result_1020_0.png\n",
      "Saved result to results1/result_1030_0.png\n",
      "Saved result to results1/result_1040_0.png\n",
      "Saved result to results1/result_1050_0.png\n",
      "Saved result to results1/result_1060_0.png\n",
      "Saved result to results1/result_1070_0.png\n",
      "Saved result to results1/result_1080_0.png\n",
      "Saved result to results1/result_1090_0.png\n",
      "Saved result to results1/result_1100_0.png\n",
      "Saved result to results1/result_1110_0.png\n",
      "Saved result to results1/result_1120_0.png\n",
      "Saved result to results1/result_1130_0.png\n",
      "Saved result to results1/result_1140_0.png\n",
      "Saved result to results1/result_1150_0.png\n",
      "Saved result to results1/result_1160_0.png\n",
      "Saved result to results1/result_1170_0.png\n",
      "Saved result to results1/result_1180_0.png\n",
      "Saved result to results1/result_1190_0.png\n",
      "Saved result to results1/result_1200_0.png\n",
      "Saved result to results1/result_1210_0.png\n",
      "Saved result to results1/result_1220_0.png\n",
      "Saved result to results1/result_1230_0.png\n",
      "Saved result to results1/result_1240_0.png\n",
      "Saved result to results1/result_1250_0.png\n",
      "Saved result to results1/result_1260_0.png\n",
      "Saved result to results1/result_1270_0.png\n",
      "Saved result to results1/result_1280_0.png\n",
      "Saved result to results1/result_1290_0.png\n",
      "Saved result to results1/result_1300_0.png\n",
      "Saved result to results1/result_1310_0.png\n",
      "Saved result to results1/result_1320_0.png\n",
      "Saved result to results1/result_1330_0.png\n",
      "Saved result to results1/result_1340_0.png\n",
      "Saved result to results1/result_1350_0.png\n",
      "Saved result to results1/result_1360_0.png\n",
      "Saved result to results1/result_1370_0.png\n",
      "Saved result to results1/result_1380_0.png\n",
      "Saved result to results1/result_1390_0.png\n",
      "Saved result to results1/result_1400_0.png\n",
      "Saved result to results1/result_1410_0.png\n",
      "Saved result to results1/result_1420_0.png\n",
      "Saved result to results1/result_1430_0.png\n",
      "Saved result to results1/result_1440_0.png\n",
      "Saved result to results1/result_1450_0.png\n",
      "Saved result to results1/result_1460_0.png\n",
      "Saved result to results1/result_1470_0.png\n",
      "Saved result to results1/result_1480_0.png\n",
      "Saved result to results1/result_1490_0.png\n",
      "Saved result to results1/result_1500_0.png\n",
      "Saved result to results1/result_1510_0.png\n",
      "Saved result to results1/result_1520_0.png\n",
      "Saved result to results1/result_1530_0.png\n",
      "Saved result to results1/result_1540_0.png\n",
      "Saved result to results1/result_1550_0.png\n",
      "Saved result to results1/result_1560_0.png\n",
      "Saved result to results1/result_1570_0.png\n",
      "Saved result to results1/result_1580_0.png\n",
      "Saved result to results1/result_1590_0.png\n",
      "Saved result to results1/result_1600_0.png\n",
      "Saved result to results1/result_1610_0.png\n",
      "Saved result to results1/result_1620_0.png\n",
      "Saved result to results1/result_1630_0.png\n",
      "Saved result to results1/result_1640_0.png\n",
      "Saved result to results1/result_1650_0.png\n",
      "Saved result to results1/result_1660_0.png\n",
      "Saved result to results1/result_1670_0.png\n",
      "Saved result to results1/result_1680_0.png\n",
      "Saved result to results1/result_1690_0.png\n",
      "Saved result to results1/result_1700_0.png\n",
      "Saved result to results1/result_1710_0.png\n",
      "Saved result to results1/result_1720_0.png\n",
      "Saved result to results1/result_1730_0.png\n",
      "Saved result to results1/result_1740_0.png\n",
      "Saved result to results1/result_1750_0.png\n",
      "Saved result to results1/result_1760_0.png\n",
      "Saved result to results1/result_1770_0.png\n",
      "Saved result to results1/result_1780_0.png\n",
      "Saved result to results1/result_1790_0.png\n",
      "Saved result to results1/result_1800_0.png\n",
      "Saved result to results1/result_1810_0.png\n",
      "Saved result to results1/result_1820_0.png\n",
      "Saved result to results1/result_1830_0.png\n",
      "Saved result to results1/result_1840_0.png\n",
      "Saved result to results1/result_1850_0.png\n",
      "Saved result to results1/result_1860_0.png\n",
      "Saved result to results1/result_1870_0.png\n",
      "Saved result to results1/result_1880_0.png\n",
      "Saved result to results1/result_1890_0.png\n",
      "Saved result to results1/result_1900_0.png\n",
      "Saved result to results1/result_1910_0.png\n",
      "Saved result to results1/result_1920_0.png\n",
      "Saved result to results1/result_1930_0.png\n",
      "Saved result to results1/result_1940_0.png\n",
      "Saved result to results1/result_1950_0.png\n",
      "Saved result to results1/result_1960_0.png\n",
      "Saved result to results1/result_1970_0.png\n",
      "Saved result to results1/result_1980_0.png\n",
      "Saved result to results1/result_1990_0.png\n",
      "Saved result to results1/result_2000_0.png\n",
      "Saved result to results1/result_2010_0.png\n",
      "Saved result to results1/result_2020_0.png\n",
      "Saved result to results1/result_2030_0.png\n",
      "Saved result to results1/result_2040_0.png\n",
      "Saved result to results1/result_2050_0.png\n",
      "Saved result to results1/result_2060_0.png\n",
      "Saved result to results1/result_2070_0.png\n",
      "Saved result to results1/result_2080_0.png\n",
      "Saved result to results1/result_2090_0.png\n",
      "Saved result to results1/result_2100_0.png\n",
      "Saved result to results1/result_2110_0.png\n",
      "Saved result to results1/result_2120_0.png\n",
      "Saved result to results1/result_2130_0.png\n",
      "Saved result to results1/result_2140_0.png\n",
      "Saved result to results1/result_2150_0.png\n",
      "Saved result to results1/result_2160_0.png\n",
      "Saved result to results1/result_2170_0.png\n",
      "Saved result to results1/result_2180_0.png\n",
      "Saved result to results1/result_2190_0.png\n",
      "Saved result to results1/result_2200_0.png\n",
      "Saved result to results1/result_2210_0.png\n",
      "Saved result to results1/result_2220_0.png\n",
      "Saved result to results1/result_2230_0.png\n",
      "Saved result to results1/result_2240_0.png\n",
      "Saved result to results1/result_2250_0.png\n",
      "Saved result to results1/result_2260_0.png\n",
      "Saved result to results1/result_2270_0.png\n",
      "Saved result to results1/result_2280_0.png\n",
      "Saved result to results1/result_2290_0.png\n",
      "Saved result to results1/result_2300_0.png\n",
      "Saved result to results1/result_2310_0.png\n",
      "Saved result to results1/result_2320_0.png\n",
      "Saved result to results1/result_2330_0.png\n",
      "Saved result to results1/result_2340_0.png\n",
      "Saved result to results1/result_2350_0.png\n",
      "Saved result to results1/result_2360_0.png\n",
      "Saved result to results1/result_2370_0.png\n",
      "Saved result to results1/result_2380_0.png\n",
      "Saved result to results1/result_2390_0.png\n",
      "Saved result to results1/result_2400_0.png\n",
      "Saved result to results1/result_2410_0.png\n",
      "Saved result to results1/result_2420_0.png\n",
      "Saved result to results1/result_2430_0.png\n",
      "Saved result to results1/result_2440_0.png\n",
      "Saved result to results1/result_2450_0.png\n",
      "\n",
      "Overall Metrics:\n",
      "Background:\n",
      "Mean DSC: 0.999\n",
      "Mean IoU: 0.999\n",
      "Kidney:\n",
      "Mean DSC: 0.926\n",
      "Mean IoU: 0.887\n",
      "Tumor:\n",
      "Mean DSC: 0.514\n",
      "Mean IoU: 0.459\n",
      "\n",
      "Overall Mean Metrics:\n",
      "Mean DSC: 0.813\n",
      "Mean IoU: 0.782\n"
     ]
    }
   ],
   "source": [
    "# Process Test Data\n",
    "all_dsc_scores = []\n",
    "all_iou_scores = []\n",
    "\n",
    "# Loop through test_loader\n",
    "for batch_idx, (image_batch, mask_batch) in enumerate(test_loader):\n",
    "    # Unpack single image and mask from the batch\n",
    "    images = image_batch.to(config[\"device\"])\n",
    "    masks = mask_batch.to(config[\"device\"]).long().squeeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get model predictions\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            prediction = predictions[i]\n",
    "            ground_truth = masks[i].cpu().numpy()\n",
    "\n",
    "            # Process and resize ground truth mask\n",
    "            ground_truth = process_mask(ground_truth, config[\"image_size\"])\n",
    "\n",
    "            # Calculate DSC and IoU\n",
    "            dsc_scores, iou_scores = calculate_metrics(prediction, ground_truth, config[\"num_classes\"])\n",
    "            all_dsc_scores.append(dsc_scores)\n",
    "            all_iou_scores.append(iou_scores)\n",
    "\n",
    "            # Visualization (optional)\n",
    "            if batch_idx % 10 == 0:  # Save every 10th image\n",
    "                colors = np.array([[0, 0, 0], [0, 255, 0], [255, 0, 0]])  # Class colors\n",
    "                prediction_rgb = colors[prediction]\n",
    "                ground_truth_rgb = colors[ground_truth]\n",
    "\n",
    "                plt.figure(figsize=(20, 8))\n",
    "                plt.subplot(131)\n",
    "                plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "                plt.title('Original Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(132)\n",
    "                plt.imshow(prediction_rgb)\n",
    "                plt.title('Prediction')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(133)\n",
    "                plt.imshow(ground_truth_rgb)\n",
    "                plt.title('Ground Truth')\n",
    "                plt.axis('off')\n",
    "\n",
    "                output_dir = \"results1\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                save_path = os.path.join(output_dir, f'result_{batch_idx}_{i}.png')\n",
    "                plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "                plt.close()\n",
    "                print(f\"Saved result to {save_path}\")\n",
    "\n",
    "# Aggregate Metrics\n",
    "if all_dsc_scores:\n",
    "    mean_dsc_per_class = [\n",
    "        np.nanmean([scores[i] for scores in all_dsc_scores if scores[i] is not None])\n",
    "        for i in range(config[\"num_classes\"])\n",
    "    ]\n",
    "    mean_iou_per_class = [\n",
    "        np.nanmean([scores[i] for scores in all_iou_scores if scores[i] is not None])\n",
    "        for i in range(config[\"num_classes\"])\n",
    "    ]\n",
    "\n",
    "    class_names = ['Background', 'Kidney', 'Tumor']\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"Mean DSC: {mean_dsc_per_class[i]:.3f}\")\n",
    "        print(f\"Mean IoU: {mean_iou_per_class[i]:.3f}\")\n",
    "\n",
    "    print(\"\\nOverall Mean Metrics:\")\n",
    "    print(f\"Mean DSC: {np.nanmean(mean_dsc_per_class):.3f}\")\n",
    "    print(f\"Mean IoU: {np.nanmean(mean_iou_per_class):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
