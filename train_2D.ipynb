{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a35TRluPFmUj",
    "outputId": "8b2825bb-8109-4341-e2c0-193e179b25dd"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.3.0 torchaudio==2.3.0 torchvision==0.18.0\n",
    "# !pip install albumentations numpy pandas scikit_learn kaggle\n",
    "# !pip install resnest geffnet opencv-python pretrainedmodels tqdm Pillow packaging monai segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1AK0H2oT0F7"
   },
   "source": [
    "Training the U-nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyw1Zn33Vq1M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import monai.networks.nets as monai_nets\n",
    "\n",
    "from utils_2D import *\n",
    "from Kits2019_2D import Kits20192DDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpAxpGnOFmUk",
    "outputId": "a3d710dd-cc81-4763-ac42-69663ca10247"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive, files\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/flattened_data.zip -d flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYGtluQ38CqG",
    "outputId": "25cd55be-9b2d-434c-eca0-b992d86a496d"
   },
   "outputs": [],
   "source": [
    "# Define the dataset configuration\n",
    "config = {\n",
    "    \"image_dir\": \"flattened_data/images\",  # Directory containing the image files\n",
    "    \"mask_dir\": \"flattened_data/masks\",   # Directory containing the mask files\n",
    "    \"split_train\": 0.7,         # Training set proportion\n",
    "    \"split_val\": 0.2,           # Validation set proportion\n",
    "    \"split_test\": 0.1,          # Testing set proportion\n",
    "    \"image_size\": 256,          # Target image size for resizing\n",
    "    \"batch_size\": 32,           # Batch size for training\n",
    "    \"num_workers\": 4,         # Number of workers for data loading\n",
    "}\n",
    "\n",
    "# Dynamically build the image and mask paths\n",
    "image_paths, mask_paths = build_dataset_paths(config[\"image_dir\"], config[\"mask_dir\"])\n",
    "\n",
    "# Update the config to include the paths\n",
    "config.update({\n",
    "    \"image_paths\": image_paths,\n",
    "    \"mask_paths\": mask_paths\n",
    "})\n",
    "\n",
    "# If in google colab, update the paths to the mounted drive\n",
    "# config[\"image_dir\"] = \"flattened_data/flattened_data/images\"\n",
    "# config[\"mask_dir\"] = \"flattened_data/flattened_data/masks\"\n",
    "\n",
    "# Prepare datasets using the updated config\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets(config)\n",
    "\n",
    "# Example output\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUOoTcoRFHEM"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXBKxQx4FmUl"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Train a MONAI model for medical image segmentation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The MONAI model to train (e.g., UNet, AttentionUNet, SwinUNETR).\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    config : dict\n",
    "        Configuration dictionary containing:\n",
    "        - \"device\" (str): Device to train on (\"cuda\" or \"cpu\").\n",
    "        - \"lr\" (float): Learning rate.\n",
    "        - \"epochs\" (int): Number of training epochs.\n",
    "        - \"checkpoint_path\" (str): Path to save the best model checkpoint.\n",
    "        - \"num_classes\" (int): Number of segmentation classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.nn.Module\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    device = config[\"device\"]\n",
    "    \n",
    "    # Wrap model with DataParallel if multiple GPUs are available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    checkpoint_path = config[\"checkpoint_path\"]\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).long().squeeze(1)  # Fixed typo in squeeze\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']} - Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device).long().squeeze(1)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{config['epochs']} - Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model with the best validation loss\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            # Save the model state dict (handle DataParallel wrapper)\n",
    "            if isinstance(model, nn.DataParallel):\n",
    "                torch.save(model.module.state_dict(), checkpoint_path)\n",
    "            else:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Model saved with validation loss: {best_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iR36AilaFmUm",
    "outputId": "2f2eee48-1642-4712-bd9e-88a3f73e3d5f"
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, config):\n",
    "    \"\"\"\n",
    "    Create DataLoaders with multi-GPU support\n",
    "    \"\"\"\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = 4 * torch.cuda.device_count() if torch.cuda.is_available() else 4  # Scale workers with GPUs\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True  # Enables faster data transfer to GPU\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(model_name, config):\n",
    "    \"\"\"\n",
    "    Initialize and return the segmentation model.\n",
    "    \"\"\"\n",
    "    if model_name == \"UNet\":\n",
    "        model = smp.Unet(\n",
    "            encoder_name=\"resnet34\",  \n",
    "            encoder_weights=\"imagenet\",   \n",
    "            in_channels=config[\"in_channels\"],  \n",
    "            classes=config[\"num_classes\"] \n",
    "        )\n",
    "    elif model_name == \"UNetV2\":\n",
    "        model = monai_nets.UNet(\n",
    "            spatial_dims=config[\"spatial_dims\"],\n",
    "            in_channels=config[\"in_channels\"],\n",
    "            out_channels=config[\"out_channels\"]\n",
    "        )\n",
    "    elif model_name == \"AttentionUNet\":\n",
    "        model = monai_nets.AttentionUnet(\n",
    "        spatial_dims=config[\"spatial_dims\"],\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        out_channels=config[\"out_channels\"],\n",
    "    )\n",
    "    elif model_name == \"SwinUNETR\":\n",
    "        model = monai_nets.SwinUNETR(\n",
    "        spatial_dims=config[\"spatial_dims\"],\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        out_channels=config[\"out_channels\"],\n",
    "        img_size=config[\"img_size\"],\n",
    "        feature_size=config[\"feature_size\"],\n",
    "    )\n",
    "    else:    \n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mask_values(dataset):\n",
    "    \"\"\"Check unique values in masks\"\"\"\n",
    "    all_unique = set()\n",
    "    for i in range(len(dataset)):\n",
    "        image, mask = dataset[i]\n",
    "        unique_values = torch.unique(mask).cpu().numpy()\n",
    "        all_unique.update(unique_values)\n",
    "    print(f\"All unique values in masks: {sorted(all_unique)}\")\n",
    "    return all_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_name = \"UNet\"\n",
    "config = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"checkpoint_path\": f\"best_{selected_model_name}.pth\",\n",
    "    \"num_classes\": 3,\n",
    "    \"in_channels\": 1,\n",
    "    \"batch_size\": 16 * torch.cuda.device_count() if torch.cuda.is_available() else 16,\n",
    "    # Add these parameters\n",
    "    \"image_dir\": \"flattened_data/images\",  # Update this path to your image directory\n",
    "    \"mask_dir\": \"flattened_data/masks\",    # Update this path to your mask directory\n",
    "    \"split_train\": 0.7,\n",
    "    \"split_val\": 0.15,\n",
    "    \"split_test\": 0.15,\n",
    "    \"image_size\": 256,\n",
    "    \"feature_size\": 48\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, mask_paths = build_dataset_paths(\n",
    "    images_dir=config[\"image_dir\"], \n",
    "    masks_dir=config[\"mask_dir\"],\n",
    "    image_ext=\".jpg\",  # Update if using different extension\n",
    "    mask_ext=\".jpg\"    # Update if using different extension\n",
    ")\n",
    "\n",
    "# Update config with the paths\n",
    "config[\"image_paths\"] = image_paths\n",
    "config[\"mask_paths\"] = mask_paths\n",
    "\n",
    "# Continue with the rest of your code\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets(config)\n",
    "\n",
    "\n",
    "# Assuming train_dataset, val_dataset, and test_dataset are already defined\n",
    "train_loader, val_loader, test_loader = create_dataloaders(train_dataset, val_dataset, test_dataset, config)\n",
    "\n",
    "\n",
    "print(\"Checking mask values...\")\n",
    "unique_train = check_mask_values(train_dataset)\n",
    "unique_val = check_mask_values(val_dataset)\n",
    "unique_test = check_mask_values(test_dataset)\n",
    "\n",
    "\n",
    "# Select and initialize the model\n",
    "\n",
    "model = select_model(selected_model_name, config)\n",
    "\n",
    "# Train the selected model\n",
    "trained_model = train_model(model, train_loader, val_loader, config)\n",
    "\n",
    "#If in Google Colab, download the trained model\n",
    "# files.download(f\"best_{selected_model_name}.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
